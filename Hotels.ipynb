{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increased-killer",
   "metadata": {},
   "source": [
    "**Hotel Review Sentiment Analysis using NLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "advisory-speaker",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:05:28.360881Z",
     "start_time": "2022-01-15T14:05:25.499866Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "respiratory-jackson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:05:57.629580Z",
     "start_time": "2022-01-15T14:05:57.607641Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "hotel_data = pd.read_csv('greek_hotel_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "empirical-slide",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:05:59.827836Z",
     "start_time": "2022-01-15T14:05:59.806865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reviews    1\n",
       "Rating     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of missing values in the dataset\n",
    "hotel_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incorporated-technology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:03.836998Z",
     "start_time": "2022-01-15T14:06:03.822037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop empty Review values\n",
    "hotel_data = hotel_data[hotel_data[\"Reviews\"].notna()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "everyday-output",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:05.503853Z",
     "start_time": "2022-01-15T14:06:05.490887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reviews    0\n",
       "Rating     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing values in the dataset\n",
    "hotel_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accomplished-income",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:07.237658Z",
     "start_time": "2022-01-15T14:06:07.180619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    5627\n",
       "4    1245\n",
       "3     455\n",
       "1     178\n",
       "2     136\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_data.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "official-afternoon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:09.586489Z",
     "start_time": "2022-01-15T14:06:09.558565Z"
    }
   },
   "outputs": [],
   "source": [
    "#Classifying reviews into “positive” and “negative” so we can use this as training data for our sentiment classification model.\n",
    "#Positive reviews will be classified as +1, and negative reviews will be classified as 0.\n",
    "\n",
    "hotel_data = hotel_data[hotel_data['Rating'] != 3]\n",
    "hotel_data['Rating'] = hotel_data['Rating'].apply(lambda rating : +1 if rating > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "behavioral-original",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:10.649622Z",
     "start_time": "2022-01-15T14:06:10.635658Z"
    }
   },
   "outputs": [],
   "source": [
    "hotel_data = hotel_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "therapeutic-station",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:12.531674Z",
     "start_time": "2022-01-15T14:06:12.515718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wonderful hotel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apanemo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best secret Getaway</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paradise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lovely place to stay!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Reviews  Rating\n",
       "0        Wonderful hotel       1\n",
       "1                Apanemo       1\n",
       "2    Best secret Getaway       1\n",
       "3               Paradise       1\n",
       "4  Lovely place to stay!       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-donna",
   "metadata": {},
   "source": [
    "# Vader knowledge based unsupervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "favorite-tiffany",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T12:50:53.704940Z",
     "start_time": "2022-01-15T12:50:52.460298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9674029212682579\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate polarity score using Vader\n",
    "def get_vader_score(sentence): \n",
    "    compound = sid.polarity_scores(sentence)['compound']\n",
    "    if compound > 0.05: \n",
    "        return 1\n",
    "    elif (compound >= -0.05) and (compound <=0.05): \n",
    "        return None\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "hotel_data['vader'] = hotel_data.apply(lambda x: get_vader_score(x['Reviews']), axis=1)\n",
    "\n",
    "# Evaluate results\n",
    "print(f'Accuracy: {accuracy_score(hotel_data.dropna()[\"Rating\"].values, hotel_data.dropna()[\"vader\"].values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-nigeria",
   "metadata": {},
   "source": [
    "# Apply TFIDF vectorization to reviews dataset and apply ML algorithms to the vectorized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-portal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T16:08:33.756325Z",
     "start_time": "2022-01-09T16:08:33.740322Z"
    }
   },
   "source": [
    "## Cleaning the text data in Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "current-nigeria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:16.168826Z",
     "start_time": "2022-01-15T14:06:16.146919Z"
    }
   },
   "outputs": [],
   "source": [
    "data = hotel_data[['Rating', 'Reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "latest-harmony",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:17.273969Z",
     "start_time": "2022-01-15T14:06:17.240064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Wonderful hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Apanemo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Best secret Getaway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Lovely place to stay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating               Reviews\n",
       "0       1       Wonderful hotel\n",
       "1       1               Apanemo\n",
       "2       1   Best secret Getaway\n",
       "3       1              Paradise\n",
       "4       1  Lovely place to stay"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the puntuation marks\n",
    "data['Reviews'] = data['Reviews'].str.replace('[^\\w\\s]','')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nasty-charge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:21.019270Z",
     "start_time": "2022-01-15T14:06:19.423103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Wonderful hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Apanemo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Best secret Getaway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Lovely place stay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating              Reviews\n",
       "0       1      Wonderful hotel\n",
       "1       1              Apanemo\n",
       "2       1  Best secret Getaway\n",
       "3       1             Paradise\n",
       "4       1    Lovely place stay"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stop_words = stopwords.words('english')\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split()\n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words\n",
    "\n",
    "data['Reviews'] = remove_stop_words(data['Reviews'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "jewish-showcase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:30.850329Z",
     "start_time": "2022-01-15T14:06:21.475761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>wonder hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>apanemo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>best secret getaway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>paradis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>love place stay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating              Reviews\n",
       "0       1         wonder hotel\n",
       "1       1              apanemo\n",
       "2       1  best secret getaway\n",
       "3       1              paradis\n",
       "4       1      love place stay"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalization to its true root\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "    \n",
    "    \n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    stem_sentence=[]\n",
    "    \n",
    "    for word, tag in tagger.tag(word_tokenize(sentence)):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else 'n'\n",
    "        word=lemmatizer.lemmatize(word,wntag)\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        \n",
    "    return \" \".join(stem_sentence)\n",
    "\n",
    "\n",
    "for index,row in data.iterrows():\n",
    "    review = stemSentence(row['Reviews'])\n",
    "    data.loc[index,'Reviews'] = review\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-singer",
   "metadata": {},
   "source": [
    "## Split the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "numerous-theta",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:32.741324Z",
     "start_time": "2022-01-15T14:06:32.724506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting train/test data\n",
    "x = data['Reviews']\n",
    "Y = data['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "narrow-designer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:34.564015Z",
     "start_time": "2022-01-15T14:06:34.526020Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, Y_train, Y_test = train_test_split(x, Y, train_size = 0.7, random_state = 500, stratify = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-bracelet",
   "metadata": {},
   "source": [
    "## Vectorization tf-idf 3-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-oakland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T16:17:27.022879Z",
     "start_time": "2022-01-09T16:17:26.958880Z"
    }
   },
   "source": [
    "TF-IDF is an abbreviation for Term Frequency Inverse Document Frequency. This is very common algorithm to transform text into a meaningful representation of numbers which is used to fit machine algorithm for prediction.\n",
    "\n",
    "Count Vectorizer give number of frequency with respect to index of vocabulary where as tf-idf consider overall documents of weight of words.\n",
    "\n",
    "![](img1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "improved-imaging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:36.325059Z",
     "start_time": "2022-01-15T14:06:36.075203Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "vectorizer.fit(xtrain)\n",
    "vectorizer.fit(xtest)\n",
    "X_train = vectorizer.transform(xtrain)\n",
    "X_test = vectorizer.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-investigator",
   "metadata": {},
   "source": [
    "Resampling involves creating a new transformed version of the training dataset in which the selected examples have a different class distribution. Random oversampling involves randomly selecting examples from the minority class, with replacement, and adding them to the training dataset. Random undersampling involves randomly selecting examples from the majority class and deleting them from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "russian-intranet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:37.868011Z",
     "start_time": "2022-01-15T14:06:37.848065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 4810, 0: 220})\n"
     ]
    }
   ],
   "source": [
    "# Check imbalanced data - summarize the class distribution\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(Y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "applicable-charity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:40.363093Z",
     "start_time": "2022-01-15T14:06:39.627703Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "rus = RandomOverSampler(random_state=777)\n",
    "\n",
    "X_train, Y_train = rus.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fewer-triple",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:06:41.585394Z",
     "start_time": "2022-01-15T14:06:41.574422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 4810, 0: 4810})\n"
     ]
    }
   ],
   "source": [
    "# summarize the new class distribution\n",
    "counter = Counter(Y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-disposal",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-tackle",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic regression is a process of modeling the probability of a discrete outcome given an input variable. The most common logistic regression models a binary outcome; something that can take two values such as true/false, yes/no, and so on. Logistic regression is a useful analysis method for classification problems, where you are trying to determine if a new sample fits best into a category. \n",
    "\n",
    "The best way to think about logistic regression is that it is a linear regression but for classification problems. Logistic regression essentially uses a logistic function defined below to model a binary output variable (Tolles & Meurer, 2016). The primary difference between linear regression and logistic regression is that logistic regression's range is bounded between 0 and 1. In addition, as opposed to linear regression, logistic regression does not require a linear relationship between inputs and output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "consolidated-shanghai",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T12:51:39.317942Z",
     "start_time": "2022-01-15T12:51:38.746683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.61      0.54        94\n",
      "           1       0.98      0.97      0.98      2062\n",
      "\n",
      "    accuracy                           0.95      2156\n",
      "   macro avg       0.73      0.79      0.76      2156\n",
      "weighted avg       0.96      0.95      0.96      2156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,Y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-tulsa",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor\n",
    "\n",
    "K-Nearest Neighbor is an exemplar-based or instance based approach which is effectively applied for sense tagged words. This classification approach based on \n",
    "instances where instances are used as points in the vector and test instance compares the new instance with all previously stored instances in the memory.\n",
    "It is based on supervised learning algorithm which is provided with the training set and during classification it compares the test instance with training set.\n",
    "\n",
    "In KNN the classification of new examples are represented in the vector form of ‘n’ features .The exemplar- based methods do not ignore any exceptions so that \n",
    "the context should be disambiguated properly.\n",
    "\n",
    "The K-Nearest Neighbor classifier selects the correct answer by comparing target word with sense inventory dictionary.\n",
    "This classifier finds the ‘k’ nearest sample to the target word and the closest sense is selected as the correct sense of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "incorporate-facial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T12:51:42.957346Z",
     "start_time": "2022-01-15T12:51:42.006843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.38      0.47        94\n",
      "           1       0.97      0.99      0.98      2062\n",
      "\n",
      "    accuracy                           0.96      2156\n",
      "   macro avg       0.80      0.69      0.73      2156\n",
      "weighted avg       0.96      0.96      0.96      2156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, Y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-study",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "The random forest algorithm is made up of a collection of decision trees, and each tree in the ensemble is comprised of a data sample drawn from a training set with replacement, called the bootstrap sample.Depending on the type of problem, the determination of the prediction will vary. For a regression task, the individual decision trees will be averaged, and for a classification task, a majority vote,this step is known as aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "logical-benjamin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T12:51:51.381571Z",
     "start_time": "2022-01-15T12:51:45.742035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.36      0.41        94\n",
      "           1       0.97      0.98      0.98      2062\n",
      "\n",
      "    accuracy                           0.96      2156\n",
      "   macro avg       0.73      0.67      0.70      2156\n",
      "weighted avg       0.95      0.96      0.95      2156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rf.fit(X_train.toarray(),Y_train)\n",
    "y_pred = rf.predict(X_test.toarray())\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-diary",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes is a family of probabilistic algorithms that take advantage of probability theory and Bayes’ Theorem to predict the target variable. They are probabilistic, which means that they calculate the probability of each tag for a given text, and then output the tag with the highest one. The way they get these probabilities is by using Bayes’ Theorem, which describes the probability of a feature, based on prior knowledge of conditions that might be related to that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fitted-consideration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T12:51:56.763832Z",
     "start_time": "2022-01-15T12:51:51.799353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.61      0.31        94\n",
      "           1       0.98      0.90      0.94      2062\n",
      "\n",
      "    accuracy                           0.88      2156\n",
      "   macro avg       0.60      0.75      0.63      2156\n",
      "weighted avg       0.95      0.88      0.91      2156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train.toarray(),Y_train)\n",
    "y_pred = nb.predict(X_test.toarray())\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-thailand",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.\n",
    "\n",
    "To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Hyperplanes are decision boundaries that help classify the data points. Our objective is to find a plane that has the maximum margin or the maximum distance between data points of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "oriented-thinking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T12:55:36.717080Z",
     "start_time": "2022-01-15T12:51:57.501232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.37      0.50        94\n",
      "           1       0.97      0.99      0.98      2062\n",
      "\n",
      "    accuracy                           0.97      2156\n",
      "   macro avg       0.87      0.68      0.74      2156\n",
      "weighted avg       0.96      0.97      0.96      2156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "sv = svm.SVC()\n",
    "sv.fit(X_train.toarray(),Y_train)\n",
    "y_pred = sv.predict(X_test.toarray())\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-handle",
   "metadata": {},
   "source": [
    "### Xgboost\n",
    "\n",
    "XGBoost the Algorithm operates on decision trees, models that construct a graph that examines the input under various \"if\" statements (vertices in the graph). Whether the \"if\" condition is satisfied influences the next \"if\" condition and eventual prediction. XGBoost the Algorithm progressively adds more and more \"if\" conditions to the decision tree to build a stronger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "eight-pricing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T12:57:37.525386Z",
     "start_time": "2022-01-15T12:57:35.999637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:57:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.49      0.53        94\n",
      "           1       0.98      0.98      0.98      2062\n",
      "\n",
      "    accuracy                           0.96      2156\n",
      "   macro avg       0.78      0.74      0.75      2156\n",
      "weighted avg       0.96      0.96      0.96      2156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,Y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-toddler",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory (LSTM) – Keras Implementation\n",
    "\n",
    "The  Long  Short-Term  Memory  or  LSTM  network  is  a  recurrent  neural  net-work that is trained using Backpropagation Through Time and overcomes thevanishing gradient problem.  LSTMs differentiated by common RNNs by usingmemory blocks that are connected into layers.  A block has components thatmake it smarter than a classical neuron and a memory for recent sequences.Thiscapability of LSTMs has been used to great effect in complex natural languageprocessing problems such as neural machine translation.\n",
    "\n",
    "**Padding**\n",
    "\n",
    "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We set the maximum size of each list to 1000. You can try a different size. The lists with size greater than 1000 will be truncated to 1000. For the lists that have length less than 1000, we will add 0 at the end of the list until it reaches the max length. This process is called padding.\n",
    "\n",
    "**Embedding**\n",
    "\n",
    "Embedding layer is one of the available layers in Keras. This is mainly used in Natural Language Processing related applications such as language modeling, but it can also be used with other tasks that involve neural networks. While dealing with NLP problems, we can use pre-trained word embeddings such as GloVe. Alternatively we can also train our own embeddings using Keras embedding layer. **Word embeddings can be thought of as an alternate to one-hot encoding along with dimensionality reduction.**\n",
    "\n",
    "Embedding layer enables us to convert each word into a fixed length vector of defined size. The resultant vector is a dense one with having real values instead of just 0’s and 1’s. The fixed length of word vectors helps us to represent words in a better way along with reduced dimensions.\n",
    "\n",
    "This way embedding layer works like a lookup table. The words are the keys in this table, while the dense word vectors are the values. To understand it better, let’s look at the implementation of Keras Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "south-angle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T12:58:04.520571Z",
     "start_time": "2022-01-15T12:58:04.408217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6675\n"
     ]
    }
   ],
   "source": [
    "tf_idf_dict = list(vectorizer.vocabulary_.items())\n",
    "\n",
    "max_val = 0\n",
    "for i in range(0,len(tf_idf_dict)-1):\n",
    "    current_val = tf_idf_dict[i][1]\n",
    "    if current_val > max_val:\n",
    "        max_val = current_val\n",
    "\n",
    "vocab_size = max_val\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "based-georgia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T12:58:40.436795Z",
     "start_time": "2022-01-15T12:58:05.387482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train.toarray(), padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test.toarray(), padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "lyric-likelihood",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T12:59:06.590475Z",
     "start_time": "2022-01-15T12:58:40.709376Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open(r'glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "northern-penny",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T13:14:59.902589Z",
     "start_time": "2022-01-15T13:14:59.190551Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in vectorizer.vocabulary_.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "single-hometown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T13:15:33.335315Z",
     "start_time": "2022-01-15T13:15:20.428833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          667500    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 784,877\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 667,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length = X_train.shape[1], weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "collective-throat",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T13:16:12.845083Z",
     "start_time": "2022-01-15T13:16:12.554007Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_nn_train = tf.one_hot(Y_train, depth=1)\n",
    "y_nn_test = tf.one_hot(Y_test, depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "proprietary-ownership",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T13:25:19.490244Z",
     "start_time": "2022-01-15T13:16:31.452646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LSTMCell._compute_carry_and_output of <keras.layers.recurrent.LSTMCell object at 0x000002D8B9B54A60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmptmlxaw85.py, line 13)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LSTMCell._compute_carry_and_output of <keras.layers.recurrent.LSTMCell object at 0x000002D8B9B54A60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmptmlxaw85.py, line 13)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Dense.call of <keras.layers.core.Dense object at 0x000002D8B9C3DFD0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpbn300an4.py, line 48)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Dense.call of <keras.layers.core.Dense object at 0x000002D8B9C3DFD0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpbn300an4.py, line 48)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "151/151 [==============================] - 143s 601ms/step - loss: 0.7050 - accuracy: 0.5020 - val_loss: 0.5922 - val_accuracy: 0.9564\n",
      "Epoch 2/5\n",
      "151/151 [==============================] - 92s 609ms/step - loss: 0.6964 - accuracy: 0.4955 - val_loss: 0.6686 - val_accuracy: 0.9564\n",
      "Epoch 3/5\n",
      "151/151 [==============================] - 95s 627ms/step - loss: 0.6955 - accuracy: 0.4932 - val_loss: 0.6604 - val_accuracy: 0.9564\n",
      "Epoch 4/5\n",
      "151/151 [==============================] - 103s 680ms/step - loss: 0.6946 - accuracy: 0.4974 - val_loss: 0.7149 - val_accuracy: 0.0436\n",
      "Epoch 5/5\n",
      "151/151 [==============================] - 90s 598ms/step - loss: 0.6951 - accuracy: 0.4873 - val_loss: 0.6961 - val_accuracy: 0.0436\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, validation_data=(X_test, y_nn_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-spending",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-15T13:27:21.307Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-street",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-15T13:27:29.314Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-automation",
   "metadata": {},
   "source": [
    "### AutoSklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "present-saturn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hydraulic-soldier",
    "outputId": "ca5b9972-92b0-40b9-b910-46e0a311b1e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.48      0.49        94\n",
      "           1       0.98      0.98      0.98      2062\n",
      "\n",
      "    accuracy                           0.96      2156\n",
      "   macro avg       0.74      0.73      0.73      2156\n",
      "weighted avg       0.96      0.96      0.96      2156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "\n",
    "X_train=X_train\n",
    "y_train=Y_train\n",
    "asc = AutoSklearnClassifier(time_left_for_this_task=10*60, n_jobs=8)\n",
    "asc.fit(X_train,y_train)\n",
    "y_pred=asc.predict(X_test)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-particle",
   "metadata": {},
   "source": [
    "# LSTM with Bidirectional Encoder Representations from Transformer BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "marked-colony",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:10:56.031924Z",
     "start_time": "2022-01-15T14:10:55.886239Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test= train_test_split(data, test_size = 0.3, random_state = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dated-novelty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:11:12.668184Z",
     "start_time": "2022-01-15T14:10:59.634169Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "reasonable-neighborhood",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:11:17.112897Z",
     "start_time": "2022-01-15T14:11:17.088965Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "  return train_InputExamples, validation_InputExamples\n",
    "\n",
    "  train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \n",
    "                                                                           test, \n",
    "                                                                           'Reviews', \n",
    "                                                                           'Rating')\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'Reviews'\n",
    "LABEL_COLUMN = 'Rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "governmental-score",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:11:28.246883Z",
     "start_time": "2022-01-15T14:11:25.858468Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "variable-round",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:20:56.462380Z",
     "start_time": "2022-01-15T14:11:30.709067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - 330s 57s/step - loss: 0.7514 - accuracy: 0.4062\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 234s 47s/step - loss: 0.1973 - accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d1c9cc6c70>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=2, steps_per_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "extensive-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T14:24:57.536530Z",
     "start_time": "2022-01-15T14:23:00.316627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        94\n",
      "           1       0.96      1.00      0.98      2062\n",
      "\n",
      "    accuracy                           0.96      2156\n",
      "   macro avg       0.48      0.50      0.49      2156\n",
      "weighted avg       0.91      0.96      0.94      2156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tf_batch = tokenizer(test['Reviews'].tolist(), max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "labels = [0,1]\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "print(classification_report(test['Rating'], label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
